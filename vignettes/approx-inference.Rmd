---
title: "Approximate Bayesian inference in `epidist`"
description: "..."
output: 
  bookdown::html_document2:
    fig_caption: yes
    code_folding: show
    number_sections: true
pkgdown:
  as_is: true
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Getting started with epidist}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include=FALSE}
# exclude compile warnings from cmdstanr
knitr::opts_chunk$set(
  fig.path = "figures/epidist-",
  cache = TRUE,
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)
```

# Background

The `epidist` package uses Bayesian inference to estimate delay distributions and other quantities.
Doing Bayesian inference amounts to approximating the posterior distribution of each parameter in the statistical model^[See a work-in-progress vignette about the model structure.].

By default, `epidist` uses the No-U-Turn Sampler [NUTS; @hoffman2014no] Hamiltonian Monte Carlo (HMC) algorithm to approximate the posterior distribution.
NUTS is an example of a broader class of Markov chain Monte Carlo (MCMC) methods.
These methods work by simulating from a Markov chain with the intended posterior distribution as its stationary distribution.
When MCMC algorithms are run for sufficiently many iterations, and have "reached convergence", the samples can be treated as being being drawn from the posterior distribution and used to compute relevant posterior quantities such as expectations.
A drawback of MCMC methods like NUTS, is that these simulations can be quite computational intensive, especially for complex models or large data.

The `epidist` package is built using `brms` [@brms], which stands for "Bayesian Regression Models using Stan", where Stan [@carpenter2017stan] is a probabilistic programming language.
NUTS is not the only inference algorithm implemented in Stan.
By relying on `brms` (and Stan) these additional inference algorithms are also available in `epidist`.
As such, if you are using `epidist` and having difficulties (for example you may have a long runtime, or your chains are failing to converge) using the default NUTS algorithm, you may want to consider an alternative.

In this vignette, we first briefly describe the alternative algorithms available (Section \@ref(other)) as well as directing you to more detailed resources.
Then (Section  \@ref(demo)) we demonstrate their application to fitting simulated data, before extracting and comparing posterior distributions.
By comparing the resulting inferences to those from NUTS, to hope to help you make informed decisions about which algorithm to use in your applied problem.

# Alternative approximate inference algorithms {#other}

Here we describe three alternative approximate Bayesian inference algorithms that are available to use in `brms`, and therefore also available in `epidist`.
It's worth noting that further inference algorithms may have become available since this vignette was last updated. 
Please check `brms` package updates if interested!

## Laplace method

The Laplace method approximates a posterior distribution by a Gaussian distribution.
In Stan, the Gaussian approximation is constructed on the unconstrained parameter space as the domain of a Gaussian distribution is the real line.
Samples from the Gaussian approximation may then be transformed to the constrained parameter space.
To access the Laplace method, specify `algorithm = "laplace"` within `brms::brm`.
See the section [Laplace Sampling](https://mc-stan.org/docs/cmdstan-guide/laplace_sample_config.html) of the `CmdStan` User's Guide for more information.

## Variational inference using ADVI

Automatic differentiation variational inference [ADVI; @kucukelbir2017advi] is a type of variational inference [VI; @blei2017variational] algorithm.
VI works by restricting to a family of distributions, and then selecting the member of that family which is the most similar to the posterior distribution.
Most commonly, and in Stan, (dis-)similarity is measured using the Kullbackâ€“Leibler divergence.
There are two options for the family of distributions, either a fully factorised Gaussian with `algorithm = "meanfield"` or a Gaussian with a full-rank covariance matrix with `algorithm = "fullrank"`.
See the section [Variational Inference using ADVI](https://mc-stan.org/docs/cmdstan-guide/variational_config.html) of the `CmdStan` User's Guide for more information.

## Pathfinder

Pathfinder is another variational inference method, more recently developed by @zhang2022pathfinder.
See the section [Pathfinder Method for Approximate Bayesian Inference](https://mc-stan.org/docs/cmdstan-guide/pathfinder_config.html) of the `CmdStan` User's Guide for more information.

# Demonstration {#demo}

In this demonstration, we use `epidist` alongside the following packages:

```{r load-requirements}
library(epidist)
library(ggplot2)
library(dplyr)
```

First, we begin by simulating data.
The example data simulation process follows that used in the [Getting started with epidist](https://epidist.epinowcast.org/articles/epidist.html#data) vignette:

```{r}
meanlog <- 1.8
sdlog <- 0.5
obs_time <- 25
sample_size <- 200

obs_cens_trunc <- simulate_gillespie(seed = 101) |>
  simulate_secondary(
    meanlog = meanlog,
    sdlog = sdlog
  ) |>
  observe_process() |>
  filter_obs_by_obs_time(obs_time = obs_time)

obs_cens_trunc_samp <-
  obs_cens_trunc[sample(seq_len(.N), sample_size, replace = FALSE)]
```

We now prepare the data for fitting with the latent individual model, and perform inference with HMC:

```{r results='hide'}
data <- epidist_prepare(obs_cens_trunc_samp, model = "latent_individual")

fit_hmc <- epidist(data = data, algorithm = "sampling")
```

Note that for clarity above we specify `algorithm = "sampling"`, but if you were to call `epidist(data = data)` the result would be the same since `"sampling"` (i.e. HMC) is the default value for the `algorithm` argument.

Now, we fit the same latent individual model using each method in Section \@ref(other).
To match the four Markov chains of length 1000 in HMC above, we then draw 4000 samples from each approximate posterior:

```{r}
fit_laplace <- epidist(data = data, algorithm = "laplace", draws = 4000)
fit_advi <- epidist(data = data, algorithm = "meanfield", draws = 4000)
fit_pathfinder <- epidist(data = data, algorithm = "pathfinder", draws = 4000)
```

Although both the Laplace and ADVI methods ran without problem, the Pathfinder algorithm produced errors of the form:

> "Error evaluating model log probability: Non-finite gradient."

We now extract posterior distribution for the delay parameters from the fitted model for each inference method:

```{r}
draws_hmc <- extract_lognormal_draws(fit_hmc)
draws_laplace <- extract_lognormal_draws(fit_laplace)
draws_advi <- extract_lognormal_draws(fit_advi)
draws_pathfinder <- extract_lognormal_draws(fit_pathfinder)
```

Compare with a figure or table or both:

```{r}
process_draws <- function(draws, name) {
  draws |>
    draws_to_long() |>
    filter(parameter %in% c("mean", "sd")) |>
    mutate(
      parameter = recode(parameter, "mean" = "Mean", "sd" = "SD"),
      method = name
    )
}

draws_hmc <- process_draws(draws_hmc, "HMC")
draws_laplace <- process_draws(draws_laplace, "Laplace")
draws_advi  <- process_draws(draws_advi, "ADVI") 
draws_pathfinder  <- process_draws(draws_pathfinder, "Pathfinder") 

df <- rbind(draws_hmc, draws_laplace, draws_advi, draws_pathfinder) |>
  mutate(method = forcats::as_factor(method))

df |>
  filter(parameter == "Mean", method != "Pathfinder") |>
  ggplot(aes(x = value)) +
  geom_histogram(aes(y = ..density..)) +
  facet_grid(method ~ parameter) +
  theme_minimal() +
  labs(x = "", y = "")

df |>
  filter(parameter == "SD", method != "Pathfinder") |>
  ggplot(aes(x = value)) +
  geom_histogram(aes(y= ..density..)) +
  facet_grid(method ~ parameter) +
  theme_minimal() +
  labs(x = "", y = "")
```

How long did each of the methods take?

```{r}
rstan::get_elapsed_time(fit_hmc$fit)
# Remains to find way to do this with other methods
```

## Bibliography {-}
