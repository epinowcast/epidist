---
title: "Using `epidist` to estimate the delay between symptom onset and positive test for the 2014-2016 Ebola outbreak in Sierra Leone"
description: "A more detailed guide to using the `epidist` R package"
output: 
  bookdown::html_document2:
    fig_caption: yes
    code_folding: show
    number_sections: true
pkgdown:
  as_is: true
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Getting in depth with epidist}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include=FALSE}
# exclude compile warnings from cmdstanr
knitr::opts_chunk$set(
  fig.path = "figures/epidist-",
  cache = TRUE,
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)
```

The 2014-2016 outbreak of Ebola in West Africa, centered on Guinea, Liberia, and Sierra Leone, was the most widespread and severe in the disease's history.
In this vignette, we use the `epidist` package to analyze data from the outbreak, as provided by @fang2016transmission.
In doing so, we demonstrate some of the more advanced features of `epidist`, such as:

1. Fitting multiple delay distributions, and selecting the best model.
2. Investigating delay estimation scenarios. A subset of the analysis in @park2024estimating.
3. Fitting location-sex stratified delay distribution estimates.

The packages used in this analysis area:

```{r load-requirements}
library(epidist)
library(data.table)
library(purrr)
library(ggplot2)
library(sf)
```

# Data preparation

We begin by loading the Ebola line list data, as provided in the `epidist` package (see `sierra_leone_ebola_data`):

```{r}
data("sierra_leone_ebola_data")

sierra_leone_ebola_data <- dplyr::select(sierra_leone_ebola_data, -name)
```

The data has `r nrow(sierra_leone_ebola_data)` rows, each corresponding to a unique case report `id`.
The other columns of the data are the individuals age, sex, the dates of symptom onset and positive sample, and their district and chiefdom.

```{r}
head(sierra_leone_ebola_data)
```

<!-- Question about whether to include shapefiles as data object in package. -->

```{r}
sf <- sf::st_read("gadm41_SLE_shp")

sierra_leone_ebola_data |>
  dplyr::group_by(chiefdom) |>
  dplyr::summarise(cases = dplyr::n()) |>
  dplyr::left_join(
    dplyr::select(sf, chiefdom = NAME_3, geometry)
  ) |>
  ggplot(aes(fill = cases, geometry = geometry)) +
    geom_sf() +
    theme_minimal() +
    labs(fill = "Cases")
```

(ref:ebola-outbreak) Figure caption.

```{r ebola-outbreak, fig.cap="(ref:ebola-outbreak)"}
# Might want to move to using data.table code rather than dplyr here/everywhere
sierra_leone_ebola_data |>
  dplyr::filter(id %% 10 == 0) |>
  ggplot() +
    geom_segment(
      aes(
        x = date_of_symptom_onset, xend = date_of_sample_tested,
        y = id, yend = id
      ),
      col = "grey"
    ) +
    geom_point(aes(x = date_of_symptom_onset, y = id), col = "#56B4E9") +
    geom_point(aes(x = date_of_sample_tested, y = id), col = "#009E73") +
    labs(x = "", y = "Case ID") +
    theme_minimal()
```

To prepare the data for use in `epidist`, we first rename the date columns as `ptime` (time of primary event) and `stime` (time of secondary event), as well as transforming to a `data.table`:

```{r}
sierra_leone_ebola_data <- sierra_leone_ebola_data |>
  dplyr::mutate(
    date_of_symptom_onset = lubridate::ymd(date_of_symptom_onset),
    date_of_sample_tested = lubridate::ymd(date_of_sample_tested),
    ptime = as.numeric(date_of_symptom_onset - min(date_of_symptom_onset)),
    stime = as.numeric(date_of_sample_tested - min(date_of_symptom_onset))
  ) |>
  dplyr::select(id, ptime, stime) |>
  data.table::as.data.table()

head(sierra_leone_ebola_data)
```

Next, we use `observe_process()` to do add interval censoring columns:

```{r}
obs_cens <- epidist::observe_process(sierra_leone_ebola_data)

head(obs_cens)

all(obs_cens$ptime == obs_cens$ptime_daily)
all(obs_cens$stime == obs_cens$stime_daily)

max(obs_cens$ptime)
max(obs_cens$stime)
```

The columns added are:

* `ptime_daily`: As the `ptime` was already daily then this is superfluous
* `ptime_lwr`:
* `ptime_upr`:
* `stime_daily`: As the `ptime` was already daily then this is superfluous
* `stime_lwr`:
* `stime_upr`:
* `delay_daily`:
* `delay_lwr`:
* `delay_upr`:
* `obs_at`: The maximum value of `stime`

Of course, to obtain the best estimate of the delay distribution now that the epidemic is concluded, we should use all of the (censored) data.
However, during a novel outbreak we do not have the luxury of waiting until the epidemic is over to begin estimating delays!
One analysis made easier by the `epidist` package is to assess how the delay distribution changes depending on the time of estimation.
Here, we consider estimation at 120 days, as well as after the epidemic is concluded at 483 days.
In a more thorough analysis, @park2024estimating also consider estimation at X, Y and Z days.

```{r}
estimation_times <- data.table(
  scenario = c("120 days", "483 days"),
  time = c(120, 483)
)

obs_cens_trunc <- purrr::pmap(estimation_times, function(scenario, time) {
  obs_cens |>
    filter_obs_by_obs_time(obs_time = time) |>
    dplyr::mutate(
      scenario = scenario,
      obs_type = "real_time"
    ) |>
    dplyr::filter(ptime_lwr >= time - 60)
})

map(obs_cens_trunc, nrow)
```

Create retrospective observations.
The point is that these don't have any truncation.
They contain all cases where the primary event happened before `time`, even if the secondary event occured after `time`.

```{r}
obs_cens_retro <- purrr::pmap(estimation_times, function(scenario, time) {
  obs_cens |>
    filter_obs_by_ptime(obs_time = time, obs_at = "max_secondary") |>
    dplyr::mutate(
      scenario = scenario,
      obs_type = "retrospective"
    ) |>
    dplyr::filter(ptime_lwr >= time - 60)
})

map(obs_cens_retro, nrow)
```

One thing we can do here is go to aggregate incidence.
Why would we want to do this?
It's used as inputs in other places.

```{r}
# inc <- event_to_incidence(obs_cens_retro, by = "scenario")
# 
# head(inc)
# dim(inc)
```

```{r}
obs_combined <- dplyr::bind_rows(
  obs_cens_trunc,
  obs_cens_retro
) |>
  dplyr::group_by(scenario, obs_type) |>
  dplyr::mutate(
    sample_size = dplyr::n()
  ) |>
  as.data.table()

nrow(obs_combined)
```

What are all the scenarios we have here?

```{r}
obs_combined |>
  dplyr::select(scenario, obs_type, sample_size) |>
  unique() |>
  dplyr::mutate(id = 1:dplyr::n())
```

# Model fitting

Fit the lognormal model(s).

```{r}
obs_combined_list <- split(obs_combined, by = c("scenario", "obs_type"))

fit_lognormal_models <- map(
  obs_combined_list,
  .f = function(x) latent_truncation_censoring_adjusted_delay(data = x)
)
```

Fit the gamma model(s).

```{r eval=FALSE}
fit_lognormal_models <- map(
  obs_combined_list,
  .f = function(x) latent_truncation_censoring_adjusted_delay(data = x, delay_prior = "gamma")
)
```

Other things here?
Convergence checking?

# Post-processing

* Extract posterior predictions (of something) from both models -- posterior predictive checking?
* Compare the models. Give suggestion of which is a better fit
* Translate fitted distribution to probability mass function

## Bibliography {-}