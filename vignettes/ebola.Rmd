---
title: "Using `epidist` to estimate reporting delays for the 2014-2016 Ebola outbreak in Sierra Leone"
description: "A more detailed guide to using the `epidist` R package"
output: 
  bookdown::html_document2:
    fig_caption: yes
    code_folding: show
    number_sections: true
pkgdown:
  as_is: true
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Getting in depth with epidist}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include=FALSE}
# exclude compile warnings from cmdstanr
knitr::opts_chunk$set(
  fig.path = "figures/epidist-",
  cache = TRUE,
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)
```

```{r load-requirements}
library(epidist)
library(data.table)
library(purrr)
library(ggplot2)
```

The 2014-2016 outbreak of Ebola in West Africa, centered on Guinea, Liberia, and Sierra Leone, was the most widespread and severe in the disease's history.
In this vignette, we use the `epidist` package to analyze data from the outbreak, as provided by @fang2016transmission.
In doing so, we demonstrate some of the more advanced features of the package, such as:

1. Fitting multiple delay distributions, and selecting the best model.
2. Investigating delay estimation scenarios. A subset of the analysis in @park2024estimating.
3. Fitting location-sex stratified delay distribution estimates.

# Data preparation

We begin by loading the Ebola linelist data, as provided in the `epidist` package (see `sierra_leone_ebola_data`):

```{r}
data("sierra_leone_ebola_data")
```

Has `r nrow(sierra_leone_ebola_data)` rows, each corresponding to a unique case report.

```{r}
head(sierra_leone_ebola_data)
```

Here add plot of cases by geography.
Question about whether to include shapefiles as data object in package.

```{r}
sf <- sf::st_read("sle_adm_gov_ocha_20231215_ab_shp")

sf |>
  dplyr::filter(admLevel == 2)
```

```{r}
# Might want to move to using data.table code rather than dplyr here/everywhere
sierra_leone_ebola_data |>
  dplyr::filter(id %% 10 == 0) |>
  ggplot() +
    geom_segment(
      aes(
        x = date_of_symptom_onset, xend = date_of_sample_tested,
        y = id, yend = id
      ),
      col = "grey"
    ) +
    geom_point(aes(x = date_of_symptom_onset, y = id), col = "#56B4E9") +
    geom_point(aes(x = date_of_sample_tested, y = id), col = "#009E73") +
    labs(x = "", y = "Case ID") +
    theme_minimal()
```

Clean it up using the paper code.
<!-- https://github.com/parksw3/epidist-paper/blob/d34a461c4d7526438d509eec99025abda30687c7/_targets.Rmd#L422 -->

First make the columns into `ptime` and `stime` and it as a `data.table`.

```{r}
sierra_leone_ebola_data <- sierra_leone_ebola_data |>
  dplyr::mutate(
    date_of_symptom_onset = lubridate::ymd(date_of_symptom_onset),
    date_of_sample_tested = lubridate::ymd(date_of_sample_tested),
    ptime = as.numeric(date_of_symptom_onset - min(date_of_symptom_onset)),
    stime = as.numeric(date_of_sample_tested - min(date_of_symptom_onset))
  ) |>
  dplyr::select(id, ptime, stime) |>
  data.table::as.data.table()

head(sierra_leone_ebola_data)
```

Now use `observe_process()` to do add interval censoring columns:

```{r}
obs_cens <- epidist::observe_process(sierra_leone_ebola_data)

head(obs_cens)

all(obs_cens$ptime == obs_cens$ptime_daily)
all(obs_cens$stime == obs_cens$stime_daily)

max(obs_cens$ptime)
max(obs_cens$stime)
```

The columns added are:

* `ptime_daily`: As the `ptime` was already daily then this is superfluous
* `ptime_lwr`:
* `ptime_upr`:
* `stime_daily`: As the `ptime` was already daily then this is superfluous
* `stime_lwr`:
* `stime_upr`:
* `delay_daily`:
* `delay_lwr`:
* `delay_upr`:
* `obs_at`: The maximum value of `stime`

Estimate at 120 days from the first cases symptom onset.
Subset of the paper (could do a larger subset).

```{r}
estimation_times <- data.table(
  scenario = c("120 days", "240 days"),
  time = c(120, 240)
)

obs_cens_trunc <- purrr::pmap(estimation_times, function(scenario, time) {
  obs_cens |>
    filter_obs_by_obs_time(obs_time = time) |>
    dplyr::mutate(
      scenario = scenario,
      obs_type = "real_time"
    ) |>
    dplyr::filter(ptime_lwr >= time - 60)
})

map(obs_cens_trunc, nrow)
```

Create retrospective observations.
The point is that these don't have any truncation.
They contain all cases where the primary event happened before `time`, even if the secondary event occured after `time`.

```{r}
obs_cens_retro <- purrr::pmap(estimation_times, function(scenario, time) {
  obs_cens |>
    filter_obs_by_ptime(obs_time = time, obs_at = "max_secondary") |>
    dplyr::mutate(
      scenario = scenario,
      obs_type = "retrospective"
    ) |>
    dplyr::filter(ptime_lwr >= time - 60)
})

map(obs_cens_retro, nrow)
```

One thing we can do here is go to aggregate incidence.
Why would we want to do this?
It's used as inputs in other places.

```{r}
# inc <- event_to_incidence(obs_cens_retro, by = "scenario")
# 
# head(inc)
# dim(inc)
```

```{r}
obs_combined <- dplyr::bind_rows(
  obs_cens_trunc,
  obs_cens_retro
) |>
  dplyr::group_by(scenario, obs_type) |>
  dplyr::mutate(
    sample_size = dplyr::n()
  ) |>
  as.data.table()

nrow(obs_combined)
```

What are all the scenarios we have here?

```{r}
obs_combined |>
  dplyr::select(scenario, obs_type, sample_size) |>
  unique() |>
  dplyr::mutate(id = 1:dplyr::n())
```

# Model fitting

Fit the lognormal model(s).

```{r}
obs_combined_list <- split(obs_combined, by = c("scenario", "obs_type"))

fit_lognormal_models <- map(
  obs_combined_list,
  .f = function(x) latent_truncation_censoring_adjusted_delay(data = x)
)
```

Fit the gamma model(s).

```{r eval=FALSE}
fit_lognormal_models <- map(
  obs_combined_list,
  .f = function(x) latent_truncation_censoring_adjusted_delay(data = x, delay_prior = "gamma")
)
```

Other things here?
Convergence checking?

# Post-processing

* Extract posterior predictions (of something) from both models -- posterior predictive checking?
* Compare the models. Give suggestion of which is a better fit
* Translate fitted distribution to probability mass function

## Bibliography {-}