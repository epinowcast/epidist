---
title: "Getting Started with Epidist"
description: "A quick start guide to using the Epidist package"
author: Epidist Team
output: 
  bookdown::html_vignette2:
    fig_caption: yes
    code_folding: show
pkgdown:
  as_is: true
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Getting Started with Epidist}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
# exclude compile warnings from cmdstanr
knitr::opts_chunk$set(
  fig.path = "figures/getting-started-nowcasting-",
  cache = TRUE, dpi = 330,
  collapse = TRUE, comment = "#>", out.width = "100%",
  message = FALSE, warning = FALSE, error = FALSE,
  eval = FALSE
)
```

# Quick start

In this quick start, we demonstrate using `epidist` to ...

# Package

In this quick start, we also use `data.table`, `purrr`, and `ggplot2` packages. These are both installed as dependencies when `epidist` is installed. Note that all output from `epidist` is readily useable with  other tools, including `tidyverse` packages (see [here](https://mgimond.github.io/rug_2019_12/Index.html) for a comparison).

```{r load-requirements}
library(epidist)
library(data.table)
library(purrr)
library(ggplot2)
```

### Simulate the data

Simulate data from an outbreak.

```{r simulate-outbreak}
outbreak <- simulate_gillespie(seed = 101)
```

Define the secondary distribution to use in the simulation

```{r secondary-dist}
secondary_dist <- data.table(
  meanlog = 1.8, sdlog = 0.5
) |>
  add_natural_scale_mean_sd()
```

Simulate an observation process during the growth phase for a secondary event using a lognormal distribution, and finally simulate observing this event.

```{r simulate-data}
obs <- outbreak |>
  simulate_secondary(
    meanlog = secondary_dist$meanlog[[1]],
    sdlog = secondary_dist$sdlog[[1]]
  ) |>
  observe_process()
```

Observe the outbreak after 25 days and take 100 samples.

```{r observe-data}
truncated_obs <- obs |>
  filter_obs_by_obs_time(obs_time = 25)
truncated_obs <- truncated_obs[sample(1:.N, 200, replace = FALSE)]
```

Plot primary cases (columns), and secondary cases (dots) by the observation time of their secondary events. This reflects would could be observed in real-time.

```{r observed-cases}
truncated_cases <- construct_cases_by_obs_window(
  obs, windows = c(25), obs_type = "stime"
)

plot_cases_by_obs_window(truncated_cases)
```

We can alternatively plot the observed data based on primary events. This corresponds to a retrospective cohort based view of the data.

```{r cohort-observed-cases}
truncated_cases <- construct_cases_by_obs_window(
  obs, windows = c(25), obs_type = "ptime"
)

plot_cases_by_obs_window(truncated_cases)
```

Plot the true continuous delay distribution and the empirical observed distribution for each observation window.

```{r empirical-dist}
combined_obs <- combine_obs(truncated_obs, obs)

plot_empirical_delay(
  combined_obs, meanlog = secondary_dist$meanlog[[1]],
  sdlog = secondary_dist$sdlog[[1]]
)
```

Plot the mean difference between continuous and discrete event time:

```{r censor_delay}
censor_delay <- calculate_censor_delay(truncated_obs)
plot_censor_delay(censor_delay)
```

### Models

First fit a naive lognormal model with no adjustment.

```{r}
naive_fit <- naive_delay(data = truncated_obs, cores = 4, refresh = 0)
```

Estimate the delay after filtering out the most recent data as crude adjustement for right truncation.

```{r}
filtered_fit <- filtered_naive_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for date censoring.

```{r}
censored_fit <- censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for censoring and filter to crudely adjust for right truncation.

```{r}
filtered_censored_fit <- filtered_censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for right truncation.

```{r}
truncation_fit <- truncation_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for right truncation and date censoring.

```{r}
truncation_censoring_fit <- truncation_censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for right truncation and date censoring using a latent variable approach.

```{r}
latent_truncation_censoring_fit <- latent_truncation_censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Fit a joint model to estimate primary incidence and the delay to reporting secondary incidence (this is a thin wrapper around the `epinowcast` package that is not suggested for real-world usage).

```{r}
epinowcast_fit <- epinowcast_delay(
  data = truncated_obs, parallel_chains = 4, adapt_delta = 0.95,
  show_messages = FALSE, refresh = 0, with_epinowcast_output = FALSE
)
epinowcast_draws <- extract_epinowcast_draws(epinowcast_fit) 
epinowcast_draws[, model := "Joint incidence and forward delay"]

```

### Summarise model posteriors and compare to known truth

Combine models into a named list.

```{r}
models <- list(
  "Naive" = naive_fit,
  "Filtered" = filtered_fit,
  "Censoring adjusted" = censored_fit,
  "Filtered and censoring adjusted" = filtered_censored_fit,
  "Truncation adjusted" = truncation_fit,
  "Truncation and censoring adjusted" = truncation_censoring_fit,
  "Latent variable truncation and censoring adjusted" =
    latent_truncation_censoring_fit
)
```

Extract and summarise lognormal posterior estimates.

```{r lognormal-draws}
draws <- models |>
  map(extract_lognormal_draws) |>
  rbindlist(idcol = "model") |>
  rbind(epinowcast_draws, use.names = TRUE)

draws <- draws[,
   model := factor(
    model, levels = c("Joint incidence and forward delay", rev(names(models)))
   )]

summarised_draws <- draws |>
  draws_to_long() |>
  summarise_draws(sf = 3)

knitr::kable(summarised_draws[parameter %in% c("meanlog", "sdlog")])
```

Plot summarised posterior estimates from each model compared to the ground truth.

```{r, fig.width = 9, fig.height = 4}
draws |>
  draws_to_long() |>
  make_relative_to_truth(draws_to_long(secondary_dist)) |>
  plot_relative_recovery(y = model, fill = model) +
  facet_wrap(vars(parameter), nrow = 1, scales = "free_x") +
  scale_fill_brewer(palette = "Dark2") +
  guides(fill = guide_none()) +
  labs(
    y = "Model", x = "Relative to ground truth"
  )
```

Finally, check the mean posterior predictions for each model against the observed daily cohort mean.
```{r postcheck}
truncated_draws <- draws |>
  calculate_truncated_means(
    obs_at = max(truncated_obs$stime_daily),
    ptime = range(truncated_obs$ptime_daily)
  ) |>
  summarise_variable(variable = "trunc_mean", by = c("obs_horizon", "model")) 

truncated_draws[, model := factor(
      model, levels = c("Joint incidence and forward delay", rev(names(models)))
    )]

truncated_draws |>
  plot_mean_posterior_pred(
    truncated_obs |>
      calculate_cohort_mean(
        type = "cohort", obs_at = max(truncated_obs$stime_daily)
      ),
    col = model, fill = model, mean = TRUE, ribbon = TRUE
  ) +
  guides(
    fill = guide_legend(title = "Model", nrow = 4),
    col = guide_legend(title = "Model", nrow = 4)
  ) +
  scale_fill_brewer(palette = "Dark2", aesthetics = c("fill", "colour")) +
  theme(legend.direction = "vertical")
```
