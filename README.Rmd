---
output: github_document
---

# Adjusting for common biases in infectious disease data when estimating distributions.


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figures/README-",
  out.width = "100%",
  dpi = 330,
  message = FALSE, warning = FALSE
)
```

## A simple example

First load our analysis (from GitHub or locally using `devtools::load_all()`) package and other required packages.

```{r load-requirements}
library(dynamicaltruncation)
library(data.table)
library(purrr)
library(ggplot2)
```

### Simulate the data

Simulate data from an outbreak.

```{r simulate-outbreak}
outbreak <- simulate_gillespie(seed = 101)
```

Define the secondary distribution to use in the simulation

```{r secondary-dist}
secondary_dist <- data.table(
  meanlog = 1.8, sdlog = 0.5
) |>
  add_natural_scale_mean_sd()
```

Simulate an observation process during the growth phase for a secondary event using a lognormal distribution, and finally simulate observing this event.

```{r simulate-data}
obs <- outbreak |>
  simulate_secondary(
    meanlog = secondary_dist$meanlog[[1]],
    sdlog = secondary_dist$sdlog[[1]]
  ) |>
  observe_process()
```

Observe the outbreak after 25 days and take 100 samples.

```{r observe-data}
truncated_obs <- obs |>
  filter_obs_by_obs_time(obs_time = 25) |>
  DT(sample(1:.N, 200, replace = FALSE))
```

Plot primary cases (columns), and secondary cases (dots) by the observation time of their secondary events. This reflects would could be observed in real-time.

```{r observed-cases}
truncated_cases <- construct_cases_by_obs_window(
  obs, windows = c(25), obs_type = "stime"
)

plot_cases_by_obs_window(truncated_cases)
```

We can alternatively plot the observed data based on primary events. This corresponds to a retrospective cohort based view of the data.

```{r cohort-observed-cases}
truncated_cases <- construct_cases_by_obs_window(
  obs, windows = c(25), obs_type = "ptime"
)

plot_cases_by_obs_window(truncated_cases)
```

Plot the true continuous delay distribution and the empirical observed distribution for each observation window.

```{r empirical-dist}
combined_obs <- combine_obs(truncated_obs, obs)

plot_empirical_delay(
  combined_obs, meanlog = secondary_dist$meanlog[[1]],
  sdlog = secondary_dist$sdlog[[1]]
)
```

Plot the mean difference between continuous and discrete event time:

```{r censor_delay}
censor_delay <- calculate_censor_delay(truncated_obs)
plot_censor_delay(censor_delay)
```

### Models

First fit a naive lognormal model with no adjustment.

```{r}
naive_fit <- naive_delay(data = truncated_obs, cores = 4, refresh = 0)
```

Estimate the delay after filtering out the most recent data as crude adjustement for right truncation.

```{r}
filtered_fit <- filtered_naive_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for date censoring.

```{r}
censored_fit <- censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for censoring and filter to crudely adjust for right truncation.

```{r}
filtered_censored_fit <- filtered_censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for right truncation.

```{r}
truncation_fit <- truncation_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for right truncation and date censoring.

```{r}
truncation_censoring_fit <- truncation_censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Adjust for right truncation and date censoring using a latent variable approach.

```{r}
latent_truncation_censoring_fit <- latent_truncation_censoring_adjusted_delay(
  data = truncated_obs, cores = 4, refresh = 0
)
```

Fit a joint model to estimate primary incidence and the delay to reporting secondary incidence (this is a thin wrapper around the `epinowcast` package that is not suggested for real-world usage).

```{r}
epinowcast_fit <- epinowcast_delay(
  data = truncated_obs, parallel_chains = 4, adapt_delta = 0.95,
  show_messages = FALSE, refresh = 0
)
epinowcast_draws <- extract_epinowcast_draws(epinowcast_fit) |>
  DT(, model := "Joint incidence and forward delay")
```

### Summarise model posteriors and compare to known truth

Combine models into a named list.

```{r}
models <- list(
  "Naive" = naive_fit,
  "Filtered" = filtered_fit,
  "Censoring adjusted" = censored_fit,
  "Filtered and censoring adjusted" = filtered_censored_fit,
  "Truncation adjusted" = truncation_fit,
  "Truncation and censoring adjusted" = truncation_censoring_fit,
  "Latent variable truncation and censoring adjusted" =
    latent_truncation_censoring_fit
)
```

Extract and summarise lognormal posterior estimates.

```{r lognormal-draws}
draws <- models |>
  map(extract_lognormal_draws) |>
  rbindlist(idcol = "model") |>
  rbind(epinowcast_draws, use.names = TRUE) |>
  DT(,
   model := factor(
    model, levels = c("Joint incidence and forward delay", rev(names(models)))
   )
  )

summarised_draws <- draws |>
  draws_to_long() |>
  summarise_draws(sf = 3)

knitr::kable(summarised_draws[parameter %in% c("meanlog", "sdlog")])
```

Plot summarised posterior estimates from each model compared to the ground truth.

```{r, fig.width = 9, fig.height = 4}
draws |>
  draws_to_long() |>
  make_relative_to_truth(draws_to_long(secondary_dist)) |>
  plot_relative_recovery(y = model, fill = model) +
  facet_wrap(vars(parameter), nrow = 1, scales = "free_x") +
  scale_fill_brewer(palette = "Dark2") +
  guides(fill = guide_none()) +
  labs(
    y = "Model", x = "Relative to ground truth"
  )
```

Finally, check the mean posterior predictions for each model against the observed daily cohort mean.
```{r postcheck}
truncated_draws <- draws |>
  calculate_truncated_means(
    obs_at = max(truncated_obs$stime_daily),
    ptime = range(truncated_obs$ptime_daily)
  ) |>
  summarise_variable(variable = "trunc_mean", by = c("obs_horizon", "model")) |>
  DT(, model := factor(
      model, levels = c("Joint incidence and forward delay", rev(names(models)))
    )
  )

truncated_draws |>
  plot_mean_posterior_pred(
    truncated_obs |>
      calculate_cohort_mean(
        type = "cohort", obs_at = max(truncated_obs$stime_daily)
      ),
    col = model, fill = model, mean = TRUE, ribbon = TRUE
  ) +
  guides(
    fill = guide_legend(title = "Model", nrow = 4),
    col = guide_legend(title = "Model", nrow = 4)
  ) +
  scale_fill_brewer(palette = "Dark2", aesthetics = c("fill", "colour")) +
  theme(legend.direction = "vertical")
```

## Analyses

This analysis in this repository has been implemented using the [`targets`](https://docs.ropensci.org/targets/) package and associated packages. The workflow is defined in [`_targets.md`](https://github.com/parksw3/dynamicaltruncation/blob/main/_targets.md) and can be explored interactively using [`_targets.Rmd`](https://github.com/parksw3/dynamicaltruncation/blob/main/_targets.Rmd) `Rmarkdown` document. The workflow can be visualised as the following graph.


This complete analysis can be recreated using the following (note this may take quite some time even with a fairly large amount of available compute),

```{bash, eval = FALSE}
bash bin/update-targets.sh
```

Alternative the following `targets` functions may be used to interactively explore the workflow:

- Run the workflow sequentially.

```{r, eval = FALSE}
targets::tar_make()
```

- Run the workflow using all available workers.

```{r, eval = FALSE}
targets::tar_make_future(workers = future::availableCores())
```

- Explore a graph of the workflow.

```{r, eval = FALSE}
targets::tar_visnetwork(targets_only = TRUE)
```

Watch the workflow as it runs in a `shiny` app.

```{r, eval = FALSE}
targets::tar_watch(targets_only = TRUE)
```

To use our archived version of the interim results (and so avoid long run times) use the following to download it. Note that this process has not been rigorously tested across environments and so may not work seamlessly).

```{r, eval = FALSE}
source(here::here("R", "targets-archive.R"))
get_targets_archive()
```