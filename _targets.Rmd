---
title: "Analysis Pipeline: Dynamic Truncation"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# Introduction

This reproducible pipeline can be used to reproduce our analysis up to fitting models to our simulated scenarios and case studies and post processing these results. This workflow uses the `targets` package and you may find reviewing an overview of how this package works (i.e in the package documentation) helpful if making use of our code. For a simplified version of our analysis that does not make use of `targets` see the repository README.

# Pipeline

The analysis pipeline for this work can be regenerated by rendering this file,

```{r, eval = FALSE}
rmarkdown::render("_targets.Rmd")
```

The pipeline can then be run using,

```{r, eval = FALSE}
tar_make()
```

The complete pipeline can be visualised using,

```{r, eval = FALSE}
tar_visnetwork()
```

Alternatively the pipeline can be explored interactively using this notebook or updated programmatically using the scripts in `bin`. We also provide an archived version of our `targets` workflow if only wanting to reproduce sections of our analysis. This can be downloaded using the following,

```{r, eval = FALSE}
source(here::here("R", "targets-archive.R"))
get_targets_archive()
```

# Setup

Set up the workflow pipeline and options. We first load the `targets` package and remove the potentially outdated workflow.

```{r}
library(targets)
library(stantargets)
library(tarchetypes)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(lubridate)
library(future)
library(future.callr)
tar_unscript()
```

We now define shared global options across our workflow and load R functions from the `R` folder.

```{targets globals, tar_globals = TRUE}
library(targets)
library(tarchetypes)
library(stantargets)
library(cmdstanr)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(future)
library(future.callr)
plan(callr)
functions <- list.files(here("R"), full.names = TRUE)
walk(functions, source)
rm("functions")
set_cmdstan_path()  

# Set the number of chains to run in parallel (more than 4 will have no impact
# on runtimes)
parallel_chains <- 4

tar_option_set(
  packages = c("data.table", "ggplot2", "purrr", "cmdstanr", "brms", "here"),
  deployment = "worker",
  memory = "transient",
  workspace_on_error = TRUE,
  error = "continue",
  garbage_collection = TRUE
)
```

# Methods

## Simulation

### Generic setup

- We assume 3 distribution scenarios: short, medium, and long.

```{targets distributions}
tar_group_by(
  distributions,
  data.table(
    scenario = c("short", "medium", "long"),
    meanlog = c(1.2, 1.6, 1.8),
    sdlog = c(0.4, 0.6, 0.8)
  ) |>
    add_natural_scale_mean_sd(),
  scenario
)
```

### Outbreak scenarios

#### Simulation

- Simulate the outbreak.

```{targets simulated_cases_outbreak, tar_simple = TRUE}
simulate_gillespie(r = 0.2, gamma = 1 / 7, init_I = 50, n = 10000, seed = 101)
```

- Simulate observations of primary and secondary events as linelist for each distribution scenario.

```{targets simulated_secondary_outbreak}
tar_target(
  simulated_secondary_outbreak, 
  simulated_cases_outbreak |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
```

- Simulate the observation process

```{targets simulated_observations_outbreak, tar_simple = TRUE}
simulated_secondary_outbreak |>
  observe_process()
```

#### Observation


- For outbreak simulations, we estimate across sample size ranges (N = 10, 100, 2000). `N = 200` is the default case

```{targets sample_sizes, tar_simple = TRUE}
c(10, 100, 200)
```


- For the outbreak simulation, we estimate all models at chosen points across the outbreak (suggestion: "early outbreak" (15 days), "near peak" (30 days), "past peak" (45  days), "late outbreak" (60 days))

```{targets outbreak_estimation_times}
tar_group_by(
  outbreak_estimation_times,
  data.table(
    scenario = c("early outbreak", "near peak", "past peak", "late outbreak"),
    time = c(15, 30, 45, 60)
  ),
  scenario
)
```

- Truncate the available simulate observations based on the estimation time for each scenario.

```{targets truncated_sim_obs_outbreak} 
tar_target(
  truncated_sim_obs_outbreak,
  simulated_observations_outbreak |>
    filter_obs_by_obs_time(
      obs_time = outbreak_estimation_times[, "time"][[1]]
    ) |>
    DT(, scenario := outbreak_estimation_times[, "scenario"][[1]]),
  pattern = map(outbreak_estimation_times)
)
```

```{targets group_truncated_sim_obs_outbreak}
tar_group_by(
  group_truncated_sim_obs_outbreak,
  truncated_sim_obs_outbreak,
  scenario, distribution
)
```

- Sample observations

```{targets sampled_simulated_observations_outbreak}
tar_target(
  sampled_simulated_observations_outbreak,
  group_truncated_sim_obs_outbreak |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, sample_sizes), replace = FALSE)) |>
    DT(, sample_size := as.factor(sample_sizes)) |>
    DT(, data_type := "outbreak"),
  pattern = cross(sample_sizes, group_truncated_sim_obs_outbreak)
)
```

```{targets list_simulated_observations_outbreak, tar_simple = TRUE}
sampled_simulated_observations_outbreak |>
  split(by = c("scenario", "distribution", "sample_size", "data_type"))
```


```{targets simulated_scenarios_outbreak, tar_simple = TRUE}
sampled_simulated_observations_outbreak |>
  DT(, .(scenario, distribution, sample_size, data_type)) |>
  unique() |>
  DT(, id := 1:.N)
```

### Exponential scenarios

#### Simulation

- We simulate scenarios in which the incidence of primary event is changing exponentially. We consider $r$ ranging from -0.2 to 0.2.

```{targets growth_rate, tar_simple=TRUE}
data.table(
    r = c(-0.2, -0.1, 0, 0.1, 0.2),
    scenario = c("fast decay", "stable", "fast growth")
  )
```

- Simulate data.

```{targets simulated_cases_exponential}
tar_target(
  simulated_cases_exponential, 
  simulate_exponential_cases(
    r = growth_rate[,"r"][[1]]
  ) |>
    DT(, r := growth_rate[,"r"][[1]]) |>
    DT(, scenario := growth_rate[,"scenario"][[1]]),
  pattern = map(growth_rate)
)
```

- Simulate observations of primary and secondary events as linelist for each distribution scenario.

```{targets simulated_secondary_exponential}
tar_target(
  simulated_secondary_exponential, 
  simulated_cases_exponential |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
```

- Simulate the observation process

```{targets simulated_observations_exponential, tar_simple = TRUE}
simulated_secondary_exponential |>
  observe_process()
```

#### Observation

- For the exponential simulation, we truncate at `t = 30`.

```{targets truncated_sim_obs_exponential} 
tar_target(
  truncated_sim_obs_exponential,
  simulated_observations_exponential |>
    filter_obs_by_obs_time(obs_time = 30) |>
    DT(, estimation_time := 30)
)
```

```{targets group_sim_obs_exponential}
tar_group_by(
  group_sim_obs_exponential,
  truncated_sim_obs_exponential,
  scenario, distribution
)
```

- Number of replicate observation processes

```{targets replicates_exponential, tar_simple = TRUE}
1:20
```

- Sample observations 

```{targets sampled_simulated_observations_exponential}
tar_target(
  sampled_simulated_observations_exponential,
  group_sim_obs_exponential |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, 200), replace = FALSE)) |>
    DT(, sample_size := as.factor(200)) |>
    DT(, data_type := "exponential") |>
    DT(, replicate := replicates_exponential),
  pattern = cross(
    sample_sizes, group_sim_obs_exponential, replicates_exponential
  )
)
```

- Group and list unique scenarios for downstream modelling.

```{targets list_simulated_observations_exponential, tar_simple = TRUE}
sampled_simulated_observations_exponential |>
  split(
    by = c("scenario", "distribution", "sample_size", "data_type", "replicate")
  )
```

```{targets simulated_scenarios_exponential, tar_simple = TRUE}
sampled_simulated_observations_exponential |>
  DT(, .(scenario, distribution, sample_size, data_type, replicate)) |>
  unique() |>
  DT(, id := 1:.N)
```

## Case study

### Data

- Case study using linelist data from ["Transmission dynamics of Ebola virus disease and intervention effectiveness in Sierra LeoneTransmission dynamics of Ebola virus disease and intervention effectiveness in Sierra Leone"](https://doi.org/10.1073/pnas.1518587113). We download and save only confirmed cases. Note this was done manually due to the journal blocking automated downloads.

```{targets raw_case_study_data, tar_simple = TRUE}
fread(here("data-raw", "pnas.1518587113.sd02.csv"))
```

- The data contains ages, sex, symptom onset date, date of sample testing, the district of the case, and the Chiefdom of the case. Here we convert these dates into the primary and secondary events `dynamicaltruncation` requires by assuming daily censoring. This means we are estimating the delay between symptom onset and a sample being tested. As we are considering overall cases only we keep only dates and our newly created delay variables.

```{targets case_study_data, tar_simple = TRUE}
raw_case_study_data |>
  DT(,
    .(
      id = 1:.N, onset_date = lubridate::dmy(`Date of symptom onset`),
      test_date = lubridate::dmy(`Date of sample tested`)
    )
  ) |>
  DT(, `:=`(
      ptime = as.numeric(onset_date - min(onset_date)),
      stime = as.numeric(test_date - min(onset_date))
    )
  ) |>
  observe_process()
```

- Save the processed data

```{targets save_case_study_data}
tar_file(
    save_case_study_data,
    save_csv(
     case_study_data, "ebola_case_study.csv", path = "data/scenarios"
    )
  )
```

### Observaton scenarios


- For our Ebola case study we estimate at 60, 120, 180, and 240 days from the first cases symptom onset.

```{targets ebola_estimation_times}
tar_group_by(
  ebola_estimation_times,
  data.table(
    scenario = c("60 days", "120 days", "180 days", "240 days"),
    time = c(60, 120, 180, 240)
  ),
  scenario
)
```

- Truncate the available simulate observations based on the estimation time for each scenario.

```{targets truncated_ebola_obs} 
tar_target(
  truncated_ebola_obs,
  case_study_data |>
    filter_obs_by_obs_time(
      obs_time = ebola_estimation_times[, "time"][[1]]
    ) |>
    DT(, scenario := ebola_estimation_times[, "scenario"][[1]]) |>
    DT(, obs_type := "real-time"),
  pattern = map(ebola_estimation_times)
)
```

- Create completely observed retrospective cohorts for the same estimation time windows.

```{targets retro_ebola_obs} 
tar_target(
  retrospective_ebola_obs,
  case_study_data |>
    filter_obs_by_ptime(
      obs_time = ebola_estimation_times[, "time"][[1]]
    ) |>
    DT(, scenario := ebola_estimation_times[, "scenario"][[1]]) |>
    DT(, obs_type := "retrospective"),
  pattern = map(ebola_estimation_times)
)
```

```{targets group_truncated_ebola_obs}
tar_group_by(
  group_truncated_ebola_obs,
  rbindlist(truncated_ebola_obs, retrospective_ebola_obs),
  scenario, obs_type
)
```

- Sample observations with 200 observations each.

```{targets sampled_ebola_observations}
tar_target(
  sampled_ebola_observations,
  group_truncated_ebola_obs |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, 200), replace = FALSE)) |>
    DT(, sample_size := 200) |>
    DT(, data_type := "ebola_case_study"),
  pattern = map(group_truncated_ebola_obs)
)
```

```{targets list_ebola_observations, tar_simple = TRUE}
sampled_ebola_observations |>
  split(by = c("scenario", "obs_type", "sample_size", "data_type"))
```

```{targets ebola_scenarios, tar_simple = TRUE}
sampled_ebola_observations |>
  DT(, .(scenario, obs_type, sample_size, data_type)) |>
  unique() |>
  DT(, id := 1:.N)
```


## Models

### Define models

We explore a range of models for estimating the log normal distribution. Starting with the naive continuous model and then gradually adding complexity to adjust for censoring to dates and right truncation. 

```{targets models, tar_globals = TRUE}
models <- list(
  "Naive" = quote(naive_delay),
  "Filtered" = quote(filtered_naive_delay),
  "Censoring adjusted" = quote(censoring_adjusted_delay),
  "Filtered and censoring adjusted" = quote(filtered_censoring_adjusted_delay),
  "Truncation adjusted" = quote(truncation_adjusted_delay),
  "Truncation and censoring adjusted" =
     quote(truncation_censoring_adjusted_delay),
  "Latent variable truncation and censoring adjusted" =
    quote(latent_truncation_censoring_adjusted_delay)
)

machine_model_names <- gsub(" ", "_", tolower(names(models)))
```

### Fit models to simulated and case study data

- Combine simulated and case study scenarios and observations

```{targets scenarios, tar_simple = TRUE}
rbind(simulated_scenarios_outbreak[, replicate := 1],
      simulated_scenarios_exponential
) |> 
  as.data.table() |> 
  DT(, id := 1:.N)
```

```{targets list_observations, tar_simple = TRUE}
c(list_simulated_observations_outbreak, list_simulated_observations_exponential)
```

- Dummy data required for model creation.

```{targets dummy_obs, tar_globals = TRUE}
dummy_obs <- data.table::data.table(
  ptime = 1, stime = 2, delay_daily = 1, delay_lwr = 1, delay_upr = 2, 
  ptime_lwr = 1, ptime_upr = 2, stime_lwr = 1, stime_upr = 2, obs_at = 100,
  censored = "interval", censored_obs_time = 10, ptime_daily = 1,
  stime_daily = 1
)
```

- Iterate over compiled models and all scenarios being investigated. For each model:
  - Create a model file
  - Generate stan code
  - Save the model to file
  - Compile the model
  - Generate stan data for each scenario
  - Fit the model to each scenario
  - Extract posterior samples for the parameters of interest
  - Summarise the posterior parameters of interest
  - Combine posterior samples and summaries with the scenarios they are linked to.
  - Summarise the model run time and other diagnostics by scenario.
  - Save posterior draws and model diagnostics

```{targets fit_models}
tar_map(
  values = list(
    model_name = machine_model_names,
    model = models
  ),
  names = model_name,
  tar_file(
    model_path,
    paste0(
      "data/models/", model_name, ".stan"
    ) |>
      fs::file_create()
  ),
  tar_target(
    model_stan_code, 
    do.call(
      model,
      list(data = dummy_obs, fn = brms::make_stancode, save_model = model_path)
    )
  ),
  tar_file(
    compiled_model_path,
    {
      stan_code <- model_stan_code
      cmdstan_model(model_path)$stan_file()
    }
  ),
  tar_target(
    standata,
    do.call(
      model,
      list(data = list_observations[[1]], fn = brms::make_standata)
    ),
    pattern = map(list_observations)
  ),
  tar_target(
    fit, 
      sample_model(
        model = compiled_model_path,
        data = standata,
        scenario = scenarios,
        adapt_delta = 0.95,
        parallel_chains = parallel_chains,
        refresh = 0, 
        show_messages = FALSE,
        iter_sampling = 1000,
        seed = 123
      ),
    pattern = map(standata, scenarios)
  ),
  tar_file(
    save_diagnostics,
    save_csv(
      fit[, -c("fit")], paste0(model_name, ".csv"), path = "data/diagnostics"
    )
  ),
  tar_target(
    draws,
    fit |>
      extract_lognormal_draws(scenarios, from_dt = TRUE) |>
      draws_to_long(),
    pattern = map(fit, scenarios)
  ),
  tar_file(
    save_lognormal_draws,
    save_rds(draws, paste0(model_name, ".rds"), path = "data/posteriors")
  ),
  tar_target(
    summarised_draws,
    summarise_lognormal_draws(draws, sf = 2)
  ),
  tar_file(
    save_summarised_draws,
    save_csv(
      summarised_draws, paste0(model_name, ".csv"),
      path = "data/summarised_posteriors"
    )
  )
)
```

## Results



