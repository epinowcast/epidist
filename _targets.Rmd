---
title: "Analysis Pipeline: Dynamic Truncation"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

# Introduction

- Reliable estimates of epidemiological distributions are required for many applications in real-time. Examples include estimates of the incubation period, the time from onset to report, and the time from onset to death. These quantities are typically assumed to follow parametric distributions but may vary over time and due to the characteristics of cases
- The data used  to estimate these distributions can suffer from a range of common biases due to censoring and truncation.
- These are... 
- These are a particular issues for infectious diseases due to the exponential nature of transmission...

- What have other people done? 
  - Examples of good practice
  - Examples of bad practice (maybe?)
  - Options for tools to address this issue


- In this study we explore the impact of these biases on naive estimates of a distribution during a simulated outbreak. We then evaluate a range of approaches to mitigate these biases and compare and contrast there performance. In particular, we focus on the role of right-truncation and dynamic adjustments to explore the relative advantages as well as if they should be used together.
We then apply these approaches to a case study.... and discuss the difference in estimates. We aim to highlight the implications of common biases found when estimating epidemiological distributions and suggest approaches to mitigate them.

# Pipeline

The analysis pipeline for this work can be regenerated by rendering this file,

```{r, eval = FALSE}
rmarkdown::render("_targets.Rmd")
```

The pipeline can then be run using,

```{r, eval = FALSE}
tar_make()
```

The complete pipeline can be visualised using,

```{r, eval = FALSE}
tar_visnetwork()
```

Alternatively the pipeline can be explored interactively using this notebook or updated programmatically using the scripts in `bin`. We also provide an archived version of our `targets` workflow if only wanting to reproduce sections of our analysis. This can be downloaded using the following,

```{r, eval = FALSE}
source(here::here("R", "targets-archive.R"))
get_targets_archive()
```

# Setup

Set up the workflow pipeline and options. We first load the `targets` package and remove the potentially outdated workflow.

```{r}
library(targets)
library(stantargets)
library(tarchetypes)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(future)
library(future.callr)
tar_unscript()
```

We now define shared global options across our workflow and load R functions from the `R` folder.

```{targets globals, tar_globals = TRUE}
library(targets)
library(tarchetypes)
library(stantargets)
library(cmdstanr)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(future)
library(future.callr)
plan(callr)
functions <- list.files(here("R"), full.names = TRUE)
walk(functions, source)
rm("functions")
set_cmdstan_path()  

tar_option_set(
  packages = c("data.table", "ggplot2", "purrr", "cmdstanr", "brms", "here"),
  deployment = "worker",
  memory = "transient",
  workspace_on_error = TRUE,
  error = "continue",
  garbage_collection = TRUE
)
```

# Methods

## Simulation

### Setup

- We assume 3 distribution scenarios: short, medium, and long.

```{targets distributions}
tar_group_by(
  distributions,
    data.table(
    scenario = c("short"),
    meanlog = c(1.2),
    sdlog = c(0.4)
  ) |>
    DT(, mean := exp(meanlog + sdlog^2/2)) |>
    DT(, sd := exp(meanlog + (1/2)*sdlog^2)*sqrt(exp(sdlog^2) - 1)),
  scenario
)
```

- We first simulate a scenario in which the incidence of primary event is changing exponentially. We consider $r$ ranging from -0.2 to 0.2 **I am now wondering if we want to investigate more than one scenario.** **such as? would exponential + outbreak be enough?**

```{targets growth_rate, tar_simple=TRUE}
data.table(
    r = c(-0.2, 0, 0.2),
    scenario = c("fast decay", "stable", "fast growth")
  )

```

Simulate data:

```{targets simulated_cases_exponential}
tar_target(
  simulated_cases_exponential, 
  simulate_exponential_cases(
    r=growth_rate[,"r"][[1]]
  ) |>
    DT(, r := growth_rate[,"r"][[1]]) |>
    DT(, scenario := growth_rate[,"scenario"][[1]]),
  pattern = map(growth_rate)
)
```

- Simulate observations of primary and secondary events as linelist for each distribution scenario.

```{targets simulated_secondary_exponential}
tar_target(
  simulated_secondary_exponential, 
  simulated_cases_exponential |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
```

- Simulate the observation process

```{targets simulated_observations_exponential, tar_simple = TRUE}
simulated_secondary_exponential |>
  observe_process()
```


- We also consider an outbreak case. Simulate the outbreak.

```{targets simulated_cases, tar_simple = TRUE}
simulate_gillespie(r = 0.2, gamma = 1 / 7, init_I = 50, n = 10000, seed = 101)
```

- Simulate observations of primary and secondary events as linelist for each distribution scenario.

```{targets simulated_secondary}
tar_target(
  simulated_secondary, 
  simulated_cases |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
```

- Simulate the observation process

```{targets simulated_observations, tar_simple = TRUE}
simulated_secondary |>
  observe_process()
```

### Estimate distributions

- For both the exponential and outbreak simulations, we estimate across sample size ranges (N = 10, 100, 1000). `N = 1000` is the main case.

```{targets sample_sizes, tar_simple = TRUE}
c(1000)
```

- For the exponential simulation, we truncate at `t = 30`

```{targets truncated_sim_obs_exponential} 
tar_target(
  truncated_sim_obs_exponential,
  simulated_observations_exponential |>
    filter_obs_by_obs_time(obs_time = 30) |>
    DT(, estimation_time := 30)
)
```

```{targets group_sim_obs_exponential}
tar_group_by(
  group_sim_obs_exponential,
  truncated_sim_obs_exponential,
  scenario, distribution
)
```

- Sample observations

```{targets sampled_simulated_observations_exponential}
tar_target(
  sampled_simulated_observations_exponential,
  group_sim_obs_exponential |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, sample_sizes), replace = FALSE)) |>
    DT(, sample_size := as.factor(sample_sizes)) |>
    DT(, datatype := "exponential"),
  pattern = cross(sample_sizes, group_sim_obs_exponential)
)
```

```{targets list_simulated_observations_exponential, tar_simple = TRUE}
sampled_simulated_observations_exponential |>
  split(by = c("scenario", "distribution", "sample_size", "datatype"))
```

```{targets simulated_scenarios_exponential, tar_simple = TRUE}
sampled_simulated_observations_exponential |>
  DT(, .(scenario, distribution, sample_size, datatype)) |>
  unique() |>
  DT(, id := 1:.N)
```

- For the outbreak simulation, we estimate all models at chosen points across the outbreak (suggestion: "early outbreak" (15 days), "near peak" (30 days), "past peak" (45  days), "late outbreak" (60 days))

```{targets estimation_times}
tar_group_by(
  estimation_times,
  data.table(
    scenario = c("early outbreak", "near peak", "past peak", "late outbreak"),
    time = c(15, 30, 45, 60)
  ),
  scenario
)
```

- Truncate the available simulate observations based on the estimation time for each scenario.

```{targets truncated_sim_obs} 
tar_target(
  truncated_sim_obs,
  simulated_observations |>
    filter_obs_by_obs_time(obs_time = estimation_times[, "time"][[1]]) |>
    DT(, scenario := estimation_times[, "scenario"][[1]]),
  pattern = map(estimation_times)
)
```

```{targets group_truncated_sim_obs}
tar_group_by(
  group_truncated_sim_obs,
  truncated_sim_obs,
  scenario, distribution
)
```

- Sample observations

```{targets sampled_simulated_observations}
tar_target(
  sampled_simulated_observations,
  group_truncated_sim_obs |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, sample_sizes), replace = FALSE)) |>
    DT(, sample_size := as.factor(sample_sizes)) |>
    DT(, datatype := "outbreak"),
  pattern = cross(sample_sizes, group_truncated_sim_obs)
)
```

```{targets list_simulated_observations, tar_simple = TRUE}
sampled_simulated_observations |>
  split(by = c("scenario", "distribution", "sample_size", "datatype"))
```


```{targets simulated_scenarios, tar_simple = TRUE}
sampled_simulated_observations |>
  DT(, .(scenario, distribution, sample_size, datatype)) |>
  unique() |>
  DT(, id := 1:.N)
```

- Plot distribution summary parameters (log mean and sd) vs true values (y facet) by outbreak point (x facet).

## Case study

### Data

- Need some outbreak linelist data.
  - Suggested (by Seb) source: 

### Estimate distributions

- Estimate distributions for similar points as in the simulation setting.

### Estimate growth rate

- Estimate the growth rate retrospectively and comment on this. 

### Summarise runtimes

- Summarise

## Models

### Define models

We explore a range of models for estimating the log normal distribution. Starting with the naive continuous model and then gradually adding complexity to adjust for censoring to dates and right truncation. 

```{targets models, tar_globals = TRUE}
models <- list(
  "Naive" = quote(naive_delay),
  "Filtered" = quote(filtered_naive_delay),
  "Censoring adjusted" = quote(censoring_adjusted_delay),
  "Filtered and censoring adjusted" = quote(filtered_censoring_adjusted_delay),
  "Truncation adjusted" = quote(truncation_adjusted_delay),
  "Truncation and censoring adjusted" =
     quote(truncation_censoring_adjusted_delay),
  "Latent variable truncation and censoring adjusted" =
    quote(latent_truncation_censoring_adjusted_delay)
)

machine_model_names <- gsub(" ", "_", tolower(names(models)))
```

### Fit models to simulated and case study data

- Combine simulated and case study scenarios and observations

```{targets scenarios, tar_simple = TRUE}
# rbind(simulated_scenarios, simulated_scenarios_exponential) |>
#   as.data.table() |> 
#   DT(, id := 1:.N)
simulated_scenarios_exponential
```

```{targets list_observations, tar_simple = TRUE}
# c(list_simulated_observations, list_simulated_observations_exponential)
list_simulated_observations_exponential
```

- Dummy data required for model creation.

```{targets dummy_obs, tar_globals = TRUE}
dummy_obs <- data.table::data.table(
  ptime = 1, stime = 2, delay_daily = 1, delay_lwr = 1, delay_upr = 2, 
  ptime_lwr = 1, ptime_upr = 2, stime_lwr = 1, stime_upr = 2, obs_at = 100,
  censored = "interval", censored_obs_time = 10, ptime_daily = 1,
  stime_daily = 1
)
```

- Iterate over compiled models and all scenarios being investigated. For each model:
  - Create a model file
  - Generate stan code
  - Save the model to file
  - Compile the model
  - Generate stan data for each scenario
  - Fit the model to each scenario
  - Extract posterior samples for the parameters of interest
  - Summarise the posterior parameters of interest
  - Combine posterior samples and summaries with the scenarios they are linked to.
  - Summarise the model run time and other diagnostics by scenario.
  - Save posterior draws and model diagnostics

```{targets fit_models}
tar_map(
  values = list(
    model_name = machine_model_names,
    model = models
  ),
  names = model_name,
  tar_file(
    model_path,
    paste0(
      "data/models/", model_name, ".stan"
    ) |>
      fs::file_create()
  ),
  tar_target(
    model_stan_code, 
    do.call(
      model,
      list(data = dummy_obs, fn = brms::make_stancode, save_model = model_path)
    )
  ),
  tar_file(
    compiled_model_path,
    {
      stan_code <- model_stan_code
      cmdstan_model(model_path)$stan_file()
    }
  ),
  tar_target(
    standata,
    do.call(
      model,
      list(data = list_observations[[1]], fn = brms::make_standata)
    ),
    pattern = map(list_observations)
  ),
  tar_target(
    fit, 
      sample_model(
        model = compiled_model_path,
        data = standata,
        scenario = scenarios,
        adapt_delta = 0.95,
        refresh = 0, 
        show_messages = FALSE,
        seed = 123
      ),
    pattern = map(standata, scenarios)
  ),
  tar_file(
    save_diagnostics,
    save_csv(
      fit[, -c("fit")], paste0(model_name, ".csv"), path = "data/diagnostics"
    )
  ),
  tar_target(
    draws,
    extract_lognormal_draws(fit, scenarios),
    pattern = map(fit, scenarios)
  ),
  tar_file(
    save_lognormal_draws,
    save_csv(draws, paste0(model_name, ".csv"), path = "data/posteriors")
  ),
  tar_target(
    summarised_draws,
    summarise_lognormal_draws(draws)
  ),
  tar_file(
    save_summarised_draws,
    save_csv(
      summarised_draws, paste0(model_name, ".csv"),
      path = "data/summarised_posteriors"
    )
  )
)
```

## Post-process for dynamic bias

- Post process all models using dynamic correction and known growth rate

## Results



