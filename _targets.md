Analysis Pipeline: Dynamic Truncation
================

# Introduction

- Reliable estimates of epidemiological distributions are required for
  many applications in real-time. Examples include estimates of the
  incubation period, the time from onset to report, and the time from
  onset to death. These quantities are typically assumed to follow
  parametric distributions but may vary over time and due to the
  characteristics of cases

- The data used to estimate these distributions can suffer from a range
  of common biases due to censoring and truncation.

- These are…

- These are a particular issues for infectious diseases due to the
  exponential nature of transmission…

- What have other people done?

  - Examples of good practice
  - Examples of bad practice (maybe?)
  - Options for tools to address this issue

- In this study we explore the impact of these biases on naive estimates
  of a distribution during a simulated outbreak. We then evaluate a
  range of approaches to mitigate these biases and compare and contrast
  there performance. In particular, we focus on the role of
  right-truncation and dynamic adjustments to explore the relative
  advantages as well as if they should be used together. We then apply
  these approaches to a case study…. and discuss the difference in
  estimates. We aim to highlight the implications of common biases found
  when estimating epidemiological distributions and suggest approaches
  to mitigate them.

# Pipeline

The analysis pipeline for this work can be regenerated by rendering this
file,

``` r
rmarkdown::render("_targets.Rmd")
```

The pipeline can then be run using,

``` r
tar_make()
```

The complete pipeline can be visualised using,

``` r
tar_visnetwork()
```

Alternatively the pipeline can be explored interactively using this
notebook or updated programmatically using the scripts in `bin`. We also
provide an archived version of our `targets` workflow if only wanting to
reproduce sections of our analysis. This can be downloaded using the
following,

``` r
source(here::here("R", "targets-archive.R"))
get_targets_archive()
```

# Setup

Set up the workflow pipeline and options. We first load the `targets`
package and remove the potentially outdated workflow.

``` r
library(targets)
library(stantargets)
library(tarchetypes)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
#> 
#> Attaching package: 'purrr'
#> The following object is masked from 'package:data.table':
#> 
#>     transpose
library(here)
#> here() starts at /workspaces/dynamicaltruncation
library(future)
library(future.callr)
tar_unscript()
```

We now define shared global options across our workflow and load R
functions from the `R` folder.

``` r
library(targets)
library(tarchetypes)
library(stantargets)
library(cmdstanr)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(future)
library(future.callr)
plan(callr)
functions <- list.files(here("R"), full.names = TRUE)
walk(functions, source)
rm("functions")
set_cmdstan_path()  

tar_option_set(
  packages = c("data.table", "ggplot2", "purrr", "cmdstanr", "brms", "here"),
  deployment = "worker",
  memory = "transient",
  workspace_on_error = TRUE,
  error = "continue",
  garbage_collection = TRUE
)
#> Establish _targets.R and _targets_r/globals/globals.R.
```

# Methods

## Simulation

### Setup

- We assume 3 distribution scenarios: short, medium, and long.

``` r
tar_group_by(
  distributions,
    data.table(
    scenario = c("short", "medium", "long"),
    meanlog = c(1.2, 1.6, 2),
    sdlog = c(0.4, 0.6, 0.8)
  ) |>
    DT(, mean := exp(meanlog + sdlog^2/2)) |>
    DT(, sd := exp(meanlog + (1/2)*sdlog^2)*sqrt(exp(sdlog^2) - 1)),
  scenario
)
#> Establish _targets.R and _targets_r/targets/distributions.R.
```

- We simulate an outbreak that starts with an initially high and stable
  growth rate that then declines linearly until reaching a stable decay
  rate. **I am now wondering if we want to investigate more than one
  scenario.**

``` r
tar_target(growth_rate, {
  data.table(
    time = 0:59,
    r = c(rep(0.2, 20), 0.2 - 0.02 * 1:20, rep(-0.2, 20))
  )
})
#> Define target growth_rate from chunk code.
#> Establish _targets.R and _targets_r/targets/growth_rate.R.
```

- We initialise the outbreak to have 20 cases.

``` r
tar_target(init_cases, {
  20
})
#> Define target init_cases from chunk code.
#> Establish _targets.R and _targets_r/targets/init_cases.R.
```

- Simulate the outbreak. This is temporary and we should fill with a
  stochastic SIR (? or other).

``` r
tar_target(simulated_cases, {
  simulate_uniform_cases(sample_size = 5000, t = 60)
})
#> Define target simulated_cases from chunk code.
#> Establish _targets.R and _targets_r/targets/simulated_cases.R.
```

- Simulate observations of primary and secondary events as linelist for
  each distribution scenario.

``` r
tar_target(
  simulated_secondary, 
  simulated_cases |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
#> Establish _targets.R and _targets_r/targets/simulated_secondary.R.
```

- Simulate the observation process

``` r
tar_target(simulated_observations, {
  simulated_secondary |>
    observe_process()
})
#> Define target simulated_observations from chunk code.
#> Establish _targets.R and _targets_r/targets/simulated_observations.R.
```

### Estimate distributions

- Estimate all models at chosen points across the outbreak (suggestion:
  “early outbreak” (15 days), “near peak” (30 days), “past peak” (45
  days), “late outbreak” (60 days))

``` r
tar_group_by(
  estimation_times,
  data.table(
    scenario = c("early outbreak", "near peak", "past peak", "late outbreak"),
    time = c(15, 30, 45, 60)
  ),
  scenario
)
#> Establish _targets.R and _targets_r/targets/estimation_times.R.
```

- Truncate the available simulate observations based on the estimation
  time for each scenario.

``` r
tar_target(
  truncated_sim_obs,
  simulated_observations |>
    filter_obs_by_obs_time(obs_time = estimation_times[, "time"][[1]]) |>
    DT(, estimation_time := estimation_times[, "scenario"][[1]]),
  pattern = map(estimation_times)
)
#> Establish _targets.R and _targets_r/targets/truncated_sim_obs.R.
```

``` r
tar_group_by(
  group_truncated_sim_obs,
  truncated_sim_obs,
  estimation_time, distribution
)
#> Establish _targets.R and _targets_r/targets/group_truncated_sim_obs.R.
```

- Estimate across sample size ranges (N = 10, 100, 1000). `N = 1000` is
  the main case.

``` r
tar_target(sample_sizes, {
  c(10, 100, 1000)
})
#> Define target sample_sizes from chunk code.
#> Establish _targets.R and _targets_r/targets/sample_sizes.R.
```

- Sample observations

``` r
tar_target(
  sampled_simulated_observations,
  group_truncated_sim_obs |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, sample_sizes), replace = FALSE)) |>
    DT(, sample_size := as.factor(sample_sizes)),
  pattern = cross(sample_sizes, group_truncated_sim_obs)
)
#> Establish _targets.R and _targets_r/targets/sampled_simulated_observations.R.
```

``` r
tar_target(list_simulated_observations, {
  sampled_simulated_observations |>
    split(by = c("estimation_time", "distribution", "sample_size"))
})
#> Define target list_simulated_observations from chunk code.
#> Establish _targets.R and _targets_r/targets/list_simulated_observations.R.
```

``` r
tar_target(simulated_scenarios, {
  sampled_simulated_observations |>
    DT(, .(estimation_time, distribution, sample_size)) |>
    unique() |>
    DT(, id := 1:.N)
})
#> Define target simulated_scenarios from chunk code.
#> Establish _targets.R and _targets_r/targets/simulated_scenarios.R.
```

- Plot distribution summary parameters (log mean and sd) vs true values
  (y facet) by outbreak point (x facet).

## Case study

### Data

- Need some outbreak linelist data. The UKHSA paper may contain this.
  - Seb suggest some Ebola linelist data that may be public domain.

### Estimate distributions

- Estimate distributions for similar points as in the simulation
  setting.

### Estimate growth rate

- Estimate the growth rate retrospectively and comment on this.

### Post-process for dynamic bias

- Apply growth rate correction to all estimates

### Summarise runtimes

- Summarise

## Models

### Define models

We explore a range of models for estimating the log normal distribution.
Starting with the naive continuous model and then gradually adding
complexity to adjust for censoring to dates and right truncation.

``` r
models <- list(
  "Naive" = quote(naive_delay),
  "Filtered" = quote(filtered_naive_delay),
  "Censoring adjusted" = quote(censoring_adjusted_delay),
  "Filtered and censoring adjusted" = quote(filtered_censoring_adjusted_delay),
  "Truncation adjusted" = quote(truncation_adjusted_delay),
  "Truncation and censoring adjusted" =
     quote(truncation_censoring_adjusted_delay),
  "Latent variable truncation and censoring adjusted" =
    quote(latent_truncation_censoring_adjusted_delay)
)

machine_model_names <- gsub(" ", "_", tolower(names(models)))
#> Establish _targets.R and _targets_r/globals/models.R.
```

### Save and compile model files

- Dummy data required for model creation.

``` r
dummy_obs <- data.table::data.table(
  ptime = 1, stime = 2, delay_daily = 1, delay_lwr = 1, delay_upr = 2, 
  ptime_lwr = 1, ptime_upr = 2, stime_lwr = 1, stime_upr = 2, obs_at = 100,
  censored = "interval", censored_obs_time = 10, ptime_daily = 1,
  stime_daily = 1
)
#> Establish _targets.R and _targets_r/globals/dummy_obs.R.
```

- Create a file for each model, generate stancode for that model, save
  the stancode to file, and finally compile the stan code to avoid
  recompilation during model fitting.

``` r
tar_map(
  values = list(model_name = machine_model_names, model = models),
  names = model_name,
  tar_file(
    model_path,
    paste0(
      "data/models/", model_name, ".stan"
    ) |>
      fs::file_create()
  ),
  tar_target(
    model_stan_code, 
    do.call(
      model,
      list(data = dummy_obs, fn = brms::make_stancode, save_model = model_path)
    )
  ),
  tar_file(
    compiled_model_path,
    {
      stan_code <- model_stan_code
      cmdstan_model(model_path)$stan_file()
    }
  )
)
#> Establish _targets.R and _targets_r/targets/save_and_compile.R.
```

### Fit models to simulated and case study data

- Combine simulated and case study scenarios and observations

``` r
tar_target(scenarios, {
  simulated_scenarios
})
#> Define target scenarios from chunk code.
#> Establish _targets.R and _targets_r/targets/scenarios.R.
```

``` r
tar_target(list_observations, {
  list_simulated_observations
})
#> Define target list_observations from chunk code.
#> Establish _targets.R and _targets_r/targets/list_observations.R.
```

- Iterate over compiled models and all scenarios being investigated. For
  each model:
  - Generate stan data for each scenario
  - Fit each the model to that scenario
  - Summarise the posterior parameters of interest
  - Extract posterior samples for the parameters of interest
  - Combine posterior samples and summaries with the scenarios they are
    linked to
  - Summarise the model run time and other diagnostics by scenario.

``` r
tar_map(
  values = list(model_name = machine_model_names, model = models),
  names = model_name,
  tar_target(
    standata,
    do.call(
      model,
      list(data = list_observations[[1]], fn = brms::make_standata)
    ),
    pattern = map(list_observations)
  )
)
#> Establish _targets.R and _targets_r/targets/fit_models.R.
```

- Create data for each group and model and then fit models to this data.

- Summarise posteriors for each model and sampled data.

- Sample model runtimes for each sampled data.

## Post-process for dynamic bias

- Post process all models using dynamic correction and known growth rate

## Results
