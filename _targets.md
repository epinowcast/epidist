Analysis Pipeline: Dynamic Truncation
================

# Introduction

This reproducible pipeline can be used to reproduce our analysis up to
fitting models to our simulated scenarios and case studies and post
processing these results. This workflow uses the `targets` package and
you may find reviewing an overview of how this package works (i.e in the
package documentation) helpful if making use of our code. For a
simplified version of our analysis that does not make use of `targets`
see the repository README.

# Pipeline

The analysis pipeline for this work can be regenerated by rendering this
file,

``` r
rmarkdown::render("_targets.Rmd")
```

The pipeline can then be run using,

``` r
tar_make()
```

The complete pipeline can be visualised using,

``` r
tar_visnetwork()
```

Alternatively the pipeline can be explored interactively using this
notebook or updated programmatically using the scripts in `bin`. We also
provide an archived version of our `targets` workflow if only wanting to
reproduce sections of our analysis. This can be downloaded using the
following,

``` r
source(here::here("R", "targets-archive.R"))
get_targets_archive()
```

# Setup

Set up the workflow pipeline and options. We first load the `targets`
package and remove the potentially outdated workflow.

``` r
library(targets)
library(stantargets)
library(tarchetypes)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(future)
library(future.callr)
tar_unscript()
```

We now define shared global options across our workflow and load R
functions from the `R` folder.

``` r
library(targets)
library(tarchetypes)
library(stantargets)
library(cmdstanr)
library(data.table)
library(ggplot2)
library(purrr, quietly = TRUE)
library(here)
library(future)
library(future.callr)
plan(callr)
functions <- list.files(here("R"), full.names = TRUE)
walk(functions, source)
rm("functions")
set_cmdstan_path()  

# Set the number of chains to run in parallel (more than 4 will have no impact
# on runtimes)
parallel_chains <- 4

tar_option_set(
  packages = c("data.table", "ggplot2", "purrr", "cmdstanr", "brms", "here"),
  deployment = "worker",
  memory = "transient",
  workspace_on_error = TRUE,
  error = "continue",
  garbage_collection = TRUE
)
```

# Methods

## Simulation

### Generic setup

- We assume 3 distribution scenarios: short, medium, and long.

``` r
tar_group_by(
  distributions,
  data.table(
    scenario = c("short", "medium", "long"),
    meanlog = c(1.2, 1.6, 1.8),
    sdlog = c(0.4, 0.6, 0.8)
  ) |>
    add_natural_scale_mean_sd(),
  scenario
)
```

### Outbreak scenarios

#### Simulation

- Simulate the outbreak.

``` r
tar_target(simulated_cases_outbreak, {
  simulate_gillespie(r = 0.2, gamma = 1 / 7, init_I = 50, n = 10000, seed = 101)
})
```

- Simulate observations of primary and secondary events as linelist for
  each distribution scenario.

``` r
tar_target(
  simulated_secondary_outbreak, 
  simulated_cases_outbreak |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
```

- Simulate the observation process

``` r
tar_target(simulated_observations_outbreak, {
  simulated_secondary_outbreak |>
    observe_process()
})
```

#### Observation

- For outbreak simulations, we estimate across sample size ranges (N =
  10, 100, 2000). `N = 200` is the default case

``` r
tar_target(sample_sizes, {
  c(10, 100, 200)
})
```

- For the outbreak simulation, we estimate all models at chosen points
  across the outbreak (suggestion: “early outbreak” (15 days), “near
  peak” (30 days), “past peak” (45 days), “late outbreak” (60 days))

``` r
tar_group_by(
  outbreak_estimation_times,
  data.table(
    scenario = c("early outbreak", "near peak", "past peak", "late outbreak"),
    time = c(15, 30, 45, 60)
  ),
  scenario
)
```

- Truncate the available simulate observations based on the estimation
  time for each scenario.

``` r
tar_target(
  truncated_sim_obs_outbreak,
  simulated_observations_outbreak |>
    filter_obs_by_obs_time(
      obs_time = outbreak_estimation_times[, "time"][[1]]
    ) |>
    DT(, scenario := outbreak_estimation_times[, "scenario"][[1]]),
  pattern = map(outbreak_estimation_times)
)
```

``` r
tar_group_by(
  group_truncated_sim_obs_outbreak,
  truncated_sim_obs_outbreak,
  scenario, distribution
)
```

- Sample observations

``` r
tar_target(
  sampled_simulated_observations_outbreak,
  group_truncated_sim_obs_outbreak |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, sample_sizes), replace = FALSE)) |>
    DT(, sample_size := as.factor(sample_sizes)) |>
    DT(, data_type := "outbreak"),
  pattern = cross(sample_sizes, group_truncated_sim_obs_outbreak)
)
```

``` r
tar_target(list_simulated_observations_outbreak, {
  sampled_simulated_observations_outbreak |>
    split(by = c("scenario", "distribution", "sample_size", "data_type"))
})
```

``` r
tar_target(simulated_scenarios_outbreak, {
  sampled_simulated_observations_outbreak |>
    DT(, .(scenario, distribution, sample_size, data_type)) |>
    unique() |>
    DT(, id := 1:.N)
})
```

### Exponential scenarios

- We simulate scenarios in which the incidence of primary event is
  changing exponentially. We consider $r$ ranging from -0.2 to 0.2.

``` r
tar_target(growth_rate, {
  data.table(
      r = c(-0.2, -0.1, 0, 0.1, 0.2),
      scenario = c("fast decay", "stable", "fast growth")
    )
})
```

- Simulate data.

``` r
tar_target(
  simulated_cases_exponential, 
  simulate_exponential_cases(
    r = growth_rate[,"r"][[1]]
  ) |>
    DT(, r := growth_rate[,"r"][[1]]) |>
    DT(, scenario := growth_rate[,"scenario"][[1]]),
  pattern = map(growth_rate)
)
```

- Simulate observations of primary and secondary events as linelist for
  each distribution scenario.

``` r
tar_target(
  simulated_secondary_exponential, 
  simulated_cases_exponential |>
    simulate_secondary(
      meanlog = distributions[, "meanlog"][[1]],
      sdlog = distributions[, "sdlog"][[1]]
    ) |>
    DT(, distribution := distributions[, "scenario"][[1]]),
  pattern = map(distributions)
)
```

- Simulate the observation process

``` r
tar_target(simulated_observations_exponential, {
  simulated_secondary_exponential |>
    observe_process()
})
```

#### Observation

- For the exponential simulation, we truncate at `t = 30`.

``` r
tar_target(
  truncated_sim_obs_exponential,
  simulated_observations_exponential |>
    filter_obs_by_obs_time(obs_time = 30) |>
    DT(, estimation_time := 30)
)
```

``` r
tar_group_by(
  group_sim_obs_exponential,
  truncated_sim_obs_exponential,
  scenario, distribution
)
```

- Number of replicate observation processes

``` r
tar_target(replicates_exponential, {
  1:20
})
```

- Sample observations

``` r
tar_target(
  sampled_simulated_observations_exponential,
  group_sim_obs_exponential |>
    as.data.table() |>
    DT(sample(1:.N, min(.N, 200), replace = FALSE)) |>
    DT(, sample_size := as.factor(200)) |>
    DT(, data_type := "exponential") |>
    DT(, replicate := replicates_exponential),
  pattern = cross(
    sample_sizes, group_sim_obs_exponential, replicates_exponential
  )
)
```

- Group and list unique scenarios for downstream modelling.

``` r
tar_target(list_simulated_observations_exponential, {
  sampled_simulated_observations_exponential |>
    split(
      by = c("scenario", "distribution", "sample_size", "data_type", "replicate")
    )
})
```

``` r
tar_target(simulated_scenarios_exponential, {
  sampled_simulated_observations_exponential |>
    DT(, .(scenario, distribution, sample_size, data_type, replicate)) |>
    unique() |>
    DT(, id := 1:.N)
})
```

## Case study

### Data

- Case study using data from [“Transmission dynamics of Ebola virus
  disease and intervention effectiveness in Sierra LeoneTransmission
  dynamics of Ebola virus disease and intervention effectiveness in
  Sierra Leone”](https://doi.org/10.1073/pnas.1518587113)

- Import data as a csv (ideally from a web URL)

- Save data

- Load saved data

- Data is for the full outbreak and has date of symptom onset and date
  of sample tested so we estimate the distribution from onset to test.

- We estimate the distribution across the outbreak at key points. 3

- Data has 3500 samples in total. We use all samples available for a
  given time period.

## Models

### Define models

We explore a range of models for estimating the log normal distribution.
Starting with the naive continuous model and then gradually adding
complexity to adjust for censoring to dates and right truncation.

``` r
models <- list(
  "Naive" = quote(naive_delay),
  "Filtered" = quote(filtered_naive_delay),
  "Censoring adjusted" = quote(censoring_adjusted_delay),
  "Filtered and censoring adjusted" = quote(filtered_censoring_adjusted_delay),
  "Truncation adjusted" = quote(truncation_adjusted_delay),
  "Truncation and censoring adjusted" =
     quote(truncation_censoring_adjusted_delay),
  "Latent variable truncation and censoring adjusted" =
    quote(latent_truncation_censoring_adjusted_delay)
)

machine_model_names <- gsub(" ", "_", tolower(names(models)))
```

### Fit models to simulated and case study data

- Combine simulated and case study scenarios and observations

``` r
tar_target(scenarios, {
  rbind(simulated_scenarios_outbreak[, replicate := 1],
        simulated_scenarios_exponential
  ) |> 
    as.data.table() |> 
    DT(, id := 1:.N)
})
```

``` r
tar_target(list_observations, {
  c(list_simulated_observations_outbreak, list_simulated_observations_exponential)
})
```

- Dummy data required for model creation.

``` r
dummy_obs <- data.table::data.table(
  ptime = 1, stime = 2, delay_daily = 1, delay_lwr = 1, delay_upr = 2, 
  ptime_lwr = 1, ptime_upr = 2, stime_lwr = 1, stime_upr = 2, obs_at = 100,
  censored = "interval", censored_obs_time = 10, ptime_daily = 1,
  stime_daily = 1
)
```

- Iterate over compiled models and all scenarios being investigated. For
  each model:
  - Create a model file
  - Generate stan code
  - Save the model to file
  - Compile the model
  - Generate stan data for each scenario
  - Fit the model to each scenario
  - Extract posterior samples for the parameters of interest
  - Summarise the posterior parameters of interest
  - Combine posterior samples and summaries with the scenarios they are
    linked to.
  - Summarise the model run time and other diagnostics by scenario.
  - Save posterior draws and model diagnostics

``` r
tar_map(
  values = list(
    model_name = machine_model_names,
    model = models
  ),
  names = model_name,
  tar_file(
    model_path,
    paste0(
      "data/models/", model_name, ".stan"
    ) |>
      fs::file_create()
  ),
  tar_target(
    model_stan_code, 
    do.call(
      model,
      list(data = dummy_obs, fn = brms::make_stancode, save_model = model_path)
    )
  ),
  tar_file(
    compiled_model_path,
    {
      stan_code <- model_stan_code
      cmdstan_model(model_path)$stan_file()
    }
  ),
  tar_target(
    standata,
    do.call(
      model,
      list(data = list_observations[[1]], fn = brms::make_standata)
    ),
    pattern = map(list_observations)
  ),
  tar_target(
    fit, 
      sample_model(
        model = compiled_model_path,
        data = standata,
        scenario = scenarios,
        adapt_delta = 0.95,
        parallel_chains = parallel_chains,
        refresh = 0, 
        show_messages = FALSE,
        iter_sampling = 1000,
        seed = 123
      ),
    pattern = map(standata, scenarios)
  ),
  tar_file(
    save_diagnostics,
    save_csv(
      fit[, -c("fit")], paste0(model_name, ".csv"), path = "data/diagnostics"
    )
  ),
  tar_target(
    draws,
    fit |>
      extract_lognormal_draws(scenarios, from_dt = TRUE) |>
      draws_to_long(),
    pattern = map(fit, scenarios)
  ),
  tar_file(
    save_lognormal_draws,
    save_rds(draws, paste0(model_name, ".rds"), path = "data/posteriors")
  ),
  tar_target(
    summarised_draws,
    summarise_lognormal_draws(draws, sf = 2)
  ),
  tar_file(
    save_summarised_draws,
    save_csv(
      summarised_draws, paste0(model_name, ".csv"),
      path = "data/summarised_posteriors"
    )
  )
)
```

## Post-process for dynamic bias

- Post process all models using dynamic correction and known growth rate

## Results
